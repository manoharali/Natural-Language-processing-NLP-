{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Natural Language Processing**\n\n## **Project:** Sentiment Analysis Using roBERTa and distilBERT\n","metadata":{}},{"cell_type":"markdown","source":"1. The `Transformers` library, developed by [Hugging Face](https://huggingface.co/docs/transformers/index) , is an open-source library designed for working with state-of-the-art natural language processing (NLP) models. \n2. `TQDM` (short for \"taqaddum\" which means \"progress\") its arabic word. It's especially useful when you're running tasks that take a long time to complete, as it give us a visual indication of the progress.","metadata":{}},{"cell_type":"code","source":"!pip install transformers scikit-learn tqdm\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-12T07:05:09.916410Z","iopub.execute_input":"2023-09-12T07:05:09.916784Z","iopub.status.idle":"2023-09-12T07:05:22.914805Z","shell.execute_reply.started":"2023-09-12T07:05:09.916750Z","shell.execute_reply":"2023-09-12T07:05:22.913592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Imported Libraries\n\n- `os`: Interacts with the operating system, providing functions for file and directory operations.\n\n- `re`: Performs regular expression operations for pattern matching or string manipulation.\n\n- `spacy`: Its NLP library offering tools and pre-trained models for tasks like `tokenization` and `entity recognition`.\n\n- `torch`: Open-source machine learning library, providing tensors.\n\n- `numpy`:  It supports array and matrix operations.\n\n- `pandas`: For data manipulation featuring data structures like data frames for structured data analysis.\n\n- `matplotlib`: A data visualization library that offers functions for creating various types of plots and charts.\n\n- `BeautifulSoup`: Facilitates web scraping by parsing HTML and XML documents, allowing extraction of relevant information.\n\n- `tqdm`: Adds progress bars to loops, providing a visual indicator of iterative process progress.\n\n- `transformers`: A library by Hugging Face for working with pre-trained transformer models in NLP, including tokenization and model loading.\n\n- `get_linear_schedule_with_warmup`: A function for scheduling learning rates during training, available in the Transformers library.\n\n- `AdamW`: Adam optimizer designed for training deep learning models, available in the Transformers library.\n\n- `DataLoader`: A PyTorch utility for loading data in batches, enabling efficient processing of large datasets during training.\n\n- `classification_report`, `confusion_matrix`, `accuracy_score`: Functions from scikit-learn for evaluating and analyzing classification model performance.\n","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport re\nimport spacy\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, get_linear_schedule_with_warmup, AdamW, DistilBertTokenizer, DistilBertForSequenceClassification\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:20:25.373858Z","iopub.execute_input":"2023-09-12T13:20:25.374534Z","iopub.status.idle":"2023-09-12T13:20:40.058319Z","shell.execute_reply.started":"2023-09-12T13:20:25.374498Z","shell.execute_reply":"2023-09-12T13:20:40.057242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Create Directories \nWe have separate folders for training and testing data, and within each of those have subfolders for positive and negative samples for extract chunks of data.\n\n1. **Directory hierarchy** \n    - `partial_data/`: This is the main directory you created.\n       * `train/`: This is the training data directory.\n            * `pos/`: This is the directory for positive training samples.\n            * `neg/`: This is the directory for negative training samples.\n       * `test/`: This is the testing data directory.\n            * `pos/`: This is the directory for positive testing samples.\n            * `neg/`: This is the directory for negative testing samples.\n","metadata":{}},{"cell_type":"code","source":"!mkdir partial_data\n!mkdir partial_data/train\n!mkdir partial_data/train/pos\n!mkdir partial_data/train/neg\n!mkdir partial_data/test\n!mkdir partial_data/test/pos\n!mkdir partial_data/test/neg","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:05:38.675668Z","iopub.execute_input":"2023-09-12T07:05:38.676370Z","iopub.status.idle":"2023-09-12T07:05:45.317181Z","shell.execute_reply.started":"2023-09-12T07:05:38.676333Z","shell.execute_reply":"2023-09-12T07:05:45.315878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ","metadata":{}},{"cell_type":"code","source":"#!rm -r /kaggle/working/partial_data","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:05:45.320821Z","iopub.execute_input":"2023-09-12T07:05:45.321413Z","iopub.status.idle":"2023-09-12T07:05:45.326616Z","shell.execute_reply.started":"2023-09-12T07:05:45.321373Z","shell.execute_reply":"2023-09-12T07:05:45.325493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Data Extracting\n\n1. **Training Data:**\n    - `2500 positive` reviews and `2500 negative` reviews are chosen for training.\n        These reviews will teach the model to recognize positive and negative sentiments.\n2. **Testing Data:**\n    - `250 positive` and `250 negative` reviews are selected for evaluating the model's performance.","metadata":{}},{"cell_type":"code","source":"!cp -t /kaggle/working/partial_data/train/neg $(ls /kaggle/input/nlp-project/aclImdb/train/neg/* | head -n 2500)\n!cp -t /kaggle/working/partial_data/train/pos $(ls /kaggle/input/nlp-project/aclImdb/train/pos/* | head -n 2500)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:05:45.328232Z","iopub.execute_input":"2023-09-12T07:05:45.328587Z","iopub.status.idle":"2023-09-12T07:07:36.679539Z","shell.execute_reply.started":"2023-09-12T07:05:45.328554Z","shell.execute_reply":"2023-09-12T07:07:36.677752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -t /kaggle/working/partial_data/test/neg $(ls /kaggle/input/nlp-project/aclImdb/test/neg/* | head -n 250)\n!cp -t /kaggle/working/partial_data/test/pos $(ls /kaggle/input/nlp-project/aclImdb/test/pos/* | head -n 250)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:07:36.681799Z","iopub.execute_input":"2023-09-12T07:07:36.682184Z","iopub.status.idle":"2023-09-12T07:08:55.356814Z","shell.execute_reply.started":"2023-09-12T07:07:36.682146Z","shell.execute_reply":"2023-09-12T07:08:55.355663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Count the text files\n\n* we define a function `count_files(directory_path)` \n\n* First checking the path is `valid` or `not` if path is valid then continue further process and take `directory/path` as      argument.\n\n* `os.lisdir(directory)` with this fucntion return the list of each filename and at the end  just return the length of list using `len(files)`.","metadata":{}},{"cell_type":"code","source":"# Function to count files in a directory\ndef count_files(directory_path):\n    if os.path.exists(directory_path):\n        # Use os.listdir() to get a list of files in the directory\n        files = os.listdir(directory_path)\n        # Use len() to get the number of files in the directory\n        return len(files)\n    else:\n        return 0  # Directory not found","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:08:55.358907Z","iopub.execute_input":"2023-09-12T07:08:55.359640Z","iopub.status.idle":"2023-09-12T07:08:55.366617Z","shell.execute_reply.started":"2023-09-12T07:08:55.359601Z","shell.execute_reply":"2023-09-12T07:08:55.365716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count files in each directory and print the results\nprint(f\"Number of files in train_positive: {count_files('/kaggle/working/partial_data/train/pos')}\")\nprint(f\"Number of files in train_negative: {count_files('/kaggle/working/partial_data/train/neg')}\")\nprint(f\"Number of files in test_positive: {count_files('/kaggle/working/partial_data/test/pos')}\")\nprint(f\"Number of files in test_negative: {count_files('/kaggle/working/partial_data/test/neg')}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:08:55.368130Z","iopub.execute_input":"2023-09-12T07:08:55.368821Z","iopub.status.idle":"2023-09-12T07:08:55.385276Z","shell.execute_reply.started":"2023-09-12T07:08:55.368787Z","shell.execute_reply":"2023-09-12T07:08:55.384222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Reading the reviews \n* We define a function `load_dataset(directory)` that take directory as argument. \n\n* Checking the each filename with `os.endswith(\".txt\")` if the filename end with `.txt` that exract it basically with this function filter out non-text files.\n\n* `open(os.path.join(directory, filename),'r')` take two things as argument one is directory and filename that exist in given directory and open the current file in `read` mode and returnt the list of reviews every review append as a string in list.","metadata":{}},{"cell_type":"code","source":"def load_dataset(directory):\n    data = []\n    # Use os.listdir() to get a list of files in the directory\n    for filename in os.listdir(directory):\n        # filter out the .txt files\n        if filename.endswith(\".txt\"):\n            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n                review = file.read()\n                data.append(review)\n    return data\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:08:55.387887Z","iopub.execute_input":"2023-09-12T07:08:55.388640Z","iopub.status.idle":"2023-09-12T07:08:55.395667Z","shell.execute_reply.started":"2023-09-12T07:08:55.388605Z","shell.execute_reply":"2023-09-12T07:08:55.394536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pos = load_dataset('/kaggle/working/partial_data/train/pos')\ntrain_neg = load_dataset('/kaggle/working/partial_data/train/neg')\ntest_pos = load_dataset('/kaggle/working/partial_data/test/pos')\ntest_neg = load_dataset('/kaggle/working/partial_data/test/neg')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:08:55.401349Z","iopub.execute_input":"2023-09-12T07:08:55.401719Z","iopub.status.idle":"2023-09-12T07:08:55.606769Z","shell.execute_reply.started":"2023-09-12T07:08:55.401660Z","shell.execute_reply":"2023-09-12T07:08:55.605798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Text preprocessing\n* we define a function `preprocess_review(review)` that take review as a argument and which is expected review in string. \n\n* checking the string is empty or not if string is empty means in which no review its simple return `empty_string` .\n\n* `BeautifulSoup(review, \"html.parser\")` BeautifulSoup is library that we used for remove the `HTML-tags` and special characters, punctuation from a review.\n\n* Using regular expressions (regex) to perform a substitution operation on the review text. `re` is refer to regex expression and `.sub()` is subsitution method search a pattern in a string and replace with another string.\n\n* `r'[^A-Za-z0-9]+` This is the regular expression pattern being used for the search operation. `^` it means 'not' or negate. `À-Za-z0-9`  its character set in whcih include uppercase `A-Z` and lowercase `a-z` and digits `0-9` . `+` means it no more accurence of the preceding pattern. \n\n* Replace with space ","metadata":{}},{"cell_type":"code","source":"def preprocess_review(review):\n    # Check if the review is not empty or None\n    if review is None or len(review) == 0:\n        return \"\"\n    \n    # Remove HTML tags and formatting\n    review = BeautifulSoup(review, \"html.parser\").get_text()\n\n    # Replace special characters, punctuation, and symbols with spaces\n    review = re.sub(r'[^A-Za-z0-9]+', ' ', review)\n\n    # Convert to lowercase\n    review = review.lower()\n\n    return review.strip()  # Remove leading and trailing spaces\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:08:55.607971Z","iopub.execute_input":"2023-09-12T07:08:55.608807Z","iopub.status.idle":"2023-09-12T07:08:55.614719Z","shell.execute_reply.started":"2023-09-12T07:08:55.608777Z","shell.execute_reply":"2023-09-12T07:08:55.613739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Calling the 'prepross_review' function.\n\nlist contain the reviews of each cateory `('train_neg', 'tarin_pos') , ('test_neg', 'test_pos')` iterate  on it and after prepross the review append in list of corresponding category. ","metadata":{}},{"cell_type":"code","source":"list_of_data = [train_pos,train_neg,test_pos,test_neg]\n\ncleaning_train_pos = []\ncleaning_train_neg = []\ncleaning_test_pos = []\ncleaning_test_neg = []\n\nfor iterate_data in list_of_data:\n    for review in iterate_data:\n        if list_of_data.index(iterate_data) == 0:\n            cleaning_train_pos.append(preprocess_review(review))\n        elif list_of_data.index(iterate_data) == 1:\n            cleaning_train_neg.append(preprocess_review(review))\n        elif list_of_data.index(iterate_data) == 2:\n            cleaning_test_pos.append(preprocess_review(review))\n        elif list_of_data.index(iterate_data) == 3:\n            cleaning_test_neg.append(preprocess_review(review))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:08:55.616328Z","iopub.execute_input":"2023-09-12T07:08:55.616962Z","iopub.status.idle":"2023-09-12T07:08:58.117531Z","shell.execute_reply.started":"2023-09-12T07:08:55.616927Z","shell.execute_reply":"2023-09-12T07:08:58.116683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Lemmatization  \n\n**Why have we preferred lemmatization and not stemming?** <br>\nBecause in **lemmatization** checking the token in vocabulary with the help of vocabulary lemmatization reduce words to their base form or root form of word. In which very low chance of loss the information. For example,**\"running\" becomes \"run,\" and \"better\" becomes \"good.\"** <br>\nbut In **stemming** also reduces words to their root form by removing prefixes or suffixes using some methods. stemming use should when less important of word meaning.In which high chances of loss the informarion. For example, **\"running\" might become \"run,\" and \"better\" might become \"better.\"**\n\n* `spaCy` is library that designed for developrs and researcher work with Natural Language Processing. It provide various facilities like **tokenization , POS, lemmatization** etc.\n\n* `spacy.load(\"en_core_web_sm\")` it loads the pre-trained English model called \"en_core_web_sm\".\n\n* `en_core_web_sm` is small size english model that trained on the web text.  It includes vocabulary, syntax, and NER.\n\n* In which function first step is create tokkens of each review and checking the each tokken from english vocabulary. and change into base/root form of word. and return the list of limmatized_review list.\n","metadata":{}},{"cell_type":"code","source":"# Load the spaCy English model\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef preprocess_and_lemmatize_review(review_text):\n    # Process the review with spaCy\n    doc = nlp(review_text)\n    \n    # Apply lemmatization and join the lemmatized words back into a string\n    lemmatized_review = \" \".join([token.lemma_ for token in doc])\n    \n    return lemmatized_review","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:08:58.118932Z","iopub.execute_input":"2023-09-12T07:08:58.119303Z","iopub.status.idle":"2023-09-12T07:08:59.320171Z","shell.execute_reply.started":"2023-09-12T07:08:58.119265Z","shell.execute_reply":"2023-09-12T07:08:59.319151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. Calling the preprocess_and_lemmatize_review() function \n\nlist contain the reviews of each cateory `('cleaning_train_neg', 'cleaning_train_pos') , ('cleaning_test_neg', 'cleaning_test_pos')` iterate  on it and after apply lemmatization on the review append in list of corresponding category. \n","metadata":{}},{"cell_type":"code","source":"cleaning_data_list = [cleaning_train_pos,cleaning_train_neg,cleaning_test_pos,cleaning_test_neg]\n\nprocessed_train_pos = []\nprocessed_train_neg = []\nprocessed_test_pos = []\nprocessed_test_neg = []\n\nfor iterate_clean_data in cleaning_data_list:\n    for movie_review in iterate_clean_data:\n        if cleaning_data_list.index(iterate_clean_data) == 0:\n            # processed_train_pos\n            processed_train_pos.append(preprocess_and_lemmatize_review(movie_review))\n            \n        elif cleaning_data_list.index(iterate_clean_data) == 1:\n            #processed_train_neg\n            processed_train_neg.append(preprocess_and_lemmatize_review(movie_review))\n            \n        elif cleaning_data_list.index(iterate_clean_data) == 2:\n            #processed_test_pos\n            processed_test_pos.append(preprocess_and_lemmatize_review(movie_review))\n            \n        elif cleaning_data_list.index(iterate_clean_data) == 3:\n            #processed_test_neg\n            processed_test_neg.append(preprocess_and_lemmatize_review(movie_review))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:08:59.321626Z","iopub.execute_input":"2023-09-12T07:08:59.322239Z","iopub.status.idle":"2023-09-12T07:12:43.468938Z","shell.execute_reply.started":"2023-09-12T07:08:59.322202Z","shell.execute_reply":"2023-09-12T07:12:43.467964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count files in each directory and print the results\nprint(f\"Number of files in processed_train_pos: {len(processed_train_pos)}\")\nprint(f\"Number of files in processed_train_neg: {len(processed_train_neg)}\")\nprint(f\"Number of files in processed_test_pos: {len(processed_test_pos)}\")\nprint(f\"Number of files in processed_test_neg: {len(processed_test_neg)}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:12:43.470303Z","iopub.execute_input":"2023-09-12T07:12:43.470746Z","iopub.status.idle":"2023-09-12T07:12:43.477295Z","shell.execute_reply.started":"2023-09-12T07:12:43.470709Z","shell.execute_reply":"2023-09-12T07:12:43.476303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10. Combine the reviews\nCombining the positive `('pos')` and negative `('neg')` reviews of train and also of test data.","metadata":{}},{"cell_type":"code","source":"train_data = processed_train_pos + processed_train_neg\ntest_data = processed_test_pos + processed_test_neg","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:12:43.478765Z","iopub.execute_input":"2023-09-12T07:12:43.479100Z","iopub.status.idle":"2023-09-12T07:12:43.488556Z","shell.execute_reply.started":"2023-09-12T07:12:43.479067Z","shell.execute_reply":"2023-09-12T07:12:43.487445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking the number of reviews in train and test data\nprint(f\"Number of files in train_data :{len(train_data)}\")\nprint(f\"Number of files in test_data :{len(test_data)}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:12:43.489806Z","iopub.execute_input":"2023-09-12T07:12:43.490242Z","iopub.status.idle":"2023-09-12T07:12:43.496704Z","shell.execute_reply.started":"2023-09-12T07:12:43.490206Z","shell.execute_reply":"2023-09-12T07:12:43.495737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 11. Create labels\nIn which we create labels corresponding each positive, negative reviews length. and combining the reviews of training and testing data. \n* The positive review label is `1` .\n* The negaive review label is `0` .\n\n**Note:** length of training and testing data should be same with training and testing labels.","metadata":{}},{"cell_type":"code","source":"# Create labels (1 for positive, 0 for negative)\ntrain_labels = [1] * len(processed_train_pos) + [0] * len(processed_train_neg)\ntest_labels = [1] * len(processed_test_pos) + [0] * len(processed_test_neg)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:12:43.498348Z","iopub.execute_input":"2023-09-12T07:12:43.499074Z","iopub.status.idle":"2023-09-12T07:12:43.504818Z","shell.execute_reply.started":"2023-09-12T07:12:43.499042Z","shell.execute_reply":"2023-09-12T07:12:43.503841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Number of files in train_labels :{len(train_labels)}\")\nprint(f\"Number of files in test_labels :{len(test_labels)}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:12:43.506339Z","iopub.execute_input":"2023-09-12T07:12:43.507154Z","iopub.status.idle":"2023-09-12T07:12:43.514078Z","shell.execute_reply.started":"2023-09-12T07:12:43.507122Z","shell.execute_reply":"2023-09-12T07:12:43.513145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 12. Overview on one review \n1. original review.\n2. cleaning review.\n3. lemmatization reviw.","metadata":{}},{"cell_type":"markdown","source":"**Original review**","metadata":{}},{"cell_type":"code","source":"review = test_pos[5]\nprint(review)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:12:43.515579Z","iopub.execute_input":"2023-09-12T07:12:43.516146Z","iopub.status.idle":"2023-09-12T07:12:43.527392Z","shell.execute_reply.started":"2023-09-12T07:12:43.516114Z","shell.execute_reply":"2023-09-12T07:12:43.526478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preprocess mean removing HTML-tags,special characters, punctuation**","metadata":{}},{"cell_type":"code","source":"processed_tes_pos = preprocess_review(review)\nprint(processed_tes_pos)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:12:43.528600Z","iopub.execute_input":"2023-09-12T07:12:43.529178Z","iopub.status.idle":"2023-09-12T07:12:43.538794Z","shell.execute_reply.started":"2023-09-12T07:12:43.529146Z","shell.execute_reply":"2023-09-12T07:12:43.537896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lemmatize review**","metadata":{}},{"cell_type":"code","source":"lemm = preprocess_and_lemmatize_review(processed_tes_pos)\nprint(lemm)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:12:43.540274Z","iopub.execute_input":"2023-09-12T07:12:43.540877Z","iopub.status.idle":"2023-09-12T07:12:43.572300Z","shell.execute_reply.started":"2023-09-12T07:12:43.540845Z","shell.execute_reply":"2023-09-12T07:12:43.571492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 13. Why we have used roBERta and distilBERT models in training?\n\n* We have tested many models like **BERT**, **mobileBERT**, **roBERTA**, **tinyBERT**,**distilBERT**, **mT5**, **XLNet**,     **BART**, **mBART**. <br>\n\n* But due to some dependencies and some variations I had to use different models. In which **BERT**, **mobileBERT**,           **roBERTa**, **distilBERT** were run successfully. So that reason we have chose these models.<br>\n\n* Some of the remaining models required more computational power because they were too large. Like **mBART** is a smaller     version of the **BART** and **BART** model also used but same memory-error and **mT5** is a smaller version of the **T5**   model. And some were not publicly available and some required private tokens.\n\n# 14. Overview of RoBERTa Model \n* **RoBERTa** stand for `Robustly Optimized BERT Pre-training Approach` it is       variant of BERT model and which was developed by Facebook AI researcher and       Washington University.<br>\n\n* It has almost **similar architecture** as compare to **BERT** but in order some improve   the results on BERT architecture.\n\n#### **Modifications to BERT:**<br>\n**->14.1. BERT uses two objectives `masked language modeling` and `next sentence pre\u0002diction.`**\n\n   * So in which remove NSP objective just use the MLM.\n   \n   - lets more explore **what is MLM(Masked language model).?**\n   \n    * MLM: Its train them to learn about contextual relationship between the word in a sentence.\n      some words in a sentence randomly masked with special token '[mask]'. **For example** we have a sentence **\"The quick       brown\n      fox jumps over the lazy dog.\"** First of all convert sentenc into tokkens and then we randomly select words that cover       with masked. Let say words is 'dog' and 'fox' create masked of it. **[\"The\", \"quick\", \"brown\", \"[MASK]\",\"jumps\", \"over\", \"the\", \"lazy\", \"[MASK]\", \".\"]** And now model predict the word behind masked\n      \n   * By learning to predict the masked words the model becomes expert at understanding the contextual relationships between       the words. it also called `Dynamic masking strategy.` <br>\n   \n   \n**->14.2. Training with bigger batch size and huge corpus**\n\n   * Original BERT is trained on 1M steps with 246 batch size but in which trained model with 124 stepd of 2K sequences and      31K steps with 8K batch-size two benefits with bigger batch size improve the perplexity of model and large batch size        is also easier to paralize.\n   \n   * It trained on 160GB text corpus that is 10 bigger than BERT train-datasize in which include different type of data like `BOOKCORPUS`, `CC-Newsx`, `OPENWEBTEXT`, `STORIES`.\n\n**->13.3. Training on large text sequence**\n\n  * In the original BERT pretraining procedure  the model observes two concatenated document seg\u0002ments, which are either         sampled contiguously from the same document. but in which `roBERRTa` is trained on `full-sentence` without NSP(next         sentence prediction).\n  \n  * **FULL-SENTENCES:** Each input is packed with full sentences sampled contiguously from one or more documents, such that     the total length is at most `512 tokens.`<br>\n\n**->14.4. Text Encoding using BPE (Byte-pair-encoding)**\n\n  * we instead consider training BERT with a larger byte-level BPE vocabulary containing `50K sub\u0002word` units, without any         additional preprocessing or tokenization of the input. but in original BERT model vocablary size is `30K`.\n  \n  \n  * This adds approxi\u0002mately 15M and 20M additional parameters for BERT-BASE and BERT-LARGE, respectively.\n  \n**Remaining architecture is same as BERT-large architecture**\n\n### Refrence:  [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)\n","metadata":{}},{"cell_type":"code","source":"#torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:12:44.606886Z","iopub.status.idle":"2023-09-12T07:12:44.607382Z","shell.execute_reply.started":"2023-09-12T07:12:44.607135Z","shell.execute_reply":"2023-09-12T07:12:44.607163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 15. Model Implementaion - roBERTa\n\n* `RobertaTokenizer` that load the pre-trained tokenizer that correspond to the `bert-base` variant. and this tokenizer is      responsible for converting text data into numerical tokens that processed the model.\n\n* we give it list of text samples that `roBERTa tokenizer` tokenize it.\n\n* `truncation=True` in which wi give a argument that is `max-length` basically maximum token length of text if the text is      above than maximum length `padding=true` add the padding in the text and pickup `512` tokkens if tokkens more than `512`    just truncate it.\n\n* `train_encodings` will contain the token IDs and attention masks for the training data.\n\n*  Dataset is created using `TensorDataset`.\n\n*  `train_encodings['input_ids']` this contain the token IDs of the training data after tokenize.\n\n* `train_encodings['attention_mask']` it contain the attention masks which indicate which tokens are actual words and which    are padding tokens.\n\n* `torch.tensor(train_labels)` This includes the labels associated with the training data. basically `TensorDataset` combine \n   the `train_encodings['input_ids']` and `train_encodings['attention_mask']`. this wrap the each sample.","metadata":{}},{"cell_type":"code","source":"tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\ntrain_encodings = tokenizer(train_data, truncation=True, padding=True, max_length=512, return_tensors='pt')\ntest_encodings = tokenizer(test_data, truncation=True, padding=True, max_length=512, return_tensors='pt')\n\ntrain_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], torch.tensor(train_labels))\ntest_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], torch.tensor(test_labels))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:14:08.020055Z","iopub.execute_input":"2023-09-12T07:14:08.020659Z","iopub.status.idle":"2023-09-12T07:14:28.616852Z","shell.execute_reply.started":"2023-09-12T07:14:08.020609Z","shell.execute_reply":"2023-09-12T07:14:28.615822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 16. Create the Dataloader - roBERTa\n- In which we set the `batch-size` to `16`. means that during the training we feed to model for processing 16 at a time.\n  * we set different batch size if we exceeding to 16 accuring the memory error your memory is not free etc. so that we set     16 samples at a time according to our computational resources.\n  \n* In first we pass wrap-up data that above created mean it`train_encodings['input_ids']` and          `train_encodings['attention_mask']` and its label.\n\n* Second we give batch-size that we define is `16`.\n\n* `shuffl=true` mean that shuffling the data at the each epochs. with this we feed whole data to the model and understand      the data contextually.","metadata":{}},{"cell_type":"code","source":"# Set up the DataLoader\nbatch_size = 16\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:14:28.619120Z","iopub.execute_input":"2023-09-12T07:14:28.619505Z","iopub.status.idle":"2023-09-12T07:14:28.627278Z","shell.execute_reply.started":"2023-09-12T07:14:28.619468Z","shell.execute_reply":"2023-09-12T07:14:28.626342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 17. Model Loading - roBERTa\n\n* load the `RobertaForSequenceClassification` pre-trained `reberta-case` model with 2 labels and tarined on own corpus.\n\n- We use `Adam` optimizer with learning rate [0.01,0.25] both lr use but not any effect on results.\n   * Adam adapts the learning rates for each parameter based on their previous gradients. It maintains a per-parameter            learning rate that is adjusted during training.\n   * It dealing with sparse or noisy gradients. so that why we use it and **original paper of roBERTa model** also use adam          optimizer.\n   \n- `torch.device` used for check the if the GPU is available and model set on device `GPU` otherwise use CPU.","metadata":{}},{"cell_type":"code","source":"# Initialize and configure the FlauBERT model for sequence classification\nmodel = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n# 1e-6\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=len(train_loader) * 1, num_training_steps=len(train_loader) * 10)\n\n# Training loop\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:14:28.628552Z","iopub.execute_input":"2023-09-12T07:14:28.629569Z","iopub.status.idle":"2023-09-12T07:14:36.634567Z","shell.execute_reply.started":"2023-09-12T07:14:28.629534Z","shell.execute_reply":"2023-09-12T07:14:36.633555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 18. Model training - roBERTa\n\n- Using loop It trains the model for the specified number of epochs.\n * intially get better result becuase we chose `10 epoch` first. we change number of 5,10,15,20 even run the `10 epochs` but not        effect on results. so that also these reason `we chose 10 epochs.`\n \n- Using the `tqdm` library visulize the model running time and running progress. \n\n- Computes training `losses` and `accuracies`.\n\n- Saves the model after each epoch.","metadata":{}},{"cell_type":"code","source":"epochs = 10\ntraining_accuracies = []\ntraining_losses = []\n\n# Create a directory to save the models\noutput_dir = \"reBERT_5e\"\nos.makedirs(output_dir, exist_ok=True)\n\nfor epoch in range(epochs):\n    model.train()\n    train_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n        input_ids, attention_mask, labels = batch\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        \n        # Use the loss from the model\n        loss = outputs.loss\n        train_loss += loss.item()\n        \n        # Compute accuracy\n        logits = outputs.logits\n        _, predicted = torch.max(logits, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        \n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n    avg_train_loss = train_loss / len(train_loader)\n    training_losses.append(avg_train_loss)\n    \n    # Calculate training accuracy\n    train_accuracy = correct / total\n    training_accuracies.append(train_accuracy)\n    \n    print(f\"Epoch {epoch + 1} - Average training loss: {avg_train_loss:.4f}, Training accuracy: {train_accuracy:.4f}\")\n    \n    # Save the model after every epoch in the specified directory\n    model.save_pretrained(os.path.join(output_dir, f\"robert_epoch_{epoch + 1}_32_adam\"))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:14:36.638400Z","iopub.execute_input":"2023-09-12T07:14:36.638704Z","iopub.status.idle":"2023-09-12T07:57:55.662894Z","shell.execute_reply.started":"2023-09-12T07:14:36.638663Z","shell.execute_reply":"2023-09-12T07:57:55.661847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Data\nepochs = range(1, len(training_accuracies) + 1)\n\n# Create subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Training Accuracy plot\nax1.plot(epochs, training_accuracies, marker='o', color='#ff7f0e', label='Training Accuracy')\nax1.set_title('roBERTa-Training Accuracy Over Epochs')\nax1.set_xlabel('Epochs')\nax1.set_ylabel('roBERTa Accuracy')\nax1.set_xticks(range(1, len(training_accuracies) + 1))\nax1.legend()\n\n# Training Loss plot\nax2.plot(epochs, training_losses, marker='o', color='#ff7f0e', label='Training Loss')\nax2.set_title('roBERTa-Training Loss Over Epochs')\nax2.set_xlabel('Epochs')\nax2.set_ylabel('roBERTA Loss')\nax2.set_xticks(range(1, len(training_accuracies) + 1))\nax2.legend()\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:53:59.212812Z","iopub.execute_input":"2023-09-12T08:53:59.213193Z","iopub.status.idle":"2023-09-12T08:53:59.881566Z","shell.execute_reply.started":"2023-09-12T08:53:59.213160Z","shell.execute_reply":"2023-09-12T08:53:59.880590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 19. Model Evaluation - roBERTa\n\n- After model is trained and **evaluate** the model that how many trained well our model.\n\n- At the end print the **Confusion Matrics**.","metadata":{}},{"cell_type":"code","source":"model.eval()\ny_true = []\ny_pred = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n        input_ids, attention_mask, labels = batch\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        predicted_labels = np.argmax(logits.cpu().numpy(), axis=1)\n\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(predicted_labels)\n\n# Print classification metrics\nprint(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Positive\"]))\nconf_matrix = confusion_matrix(y_true, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:05:05.219314Z","iopub.execute_input":"2023-09-12T08:05:05.219740Z","iopub.status.idle":"2023-09-12T08:05:13.848495Z","shell.execute_reply.started":"2023-09-12T08:05:05.219678Z","shell.execute_reply":"2023-09-12T08:05:13.847588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#torch.cuda.empty_cache()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:12:44.629654Z","iopub.status.idle":"2023-09-12T07:12:44.630166Z","shell.execute_reply.started":"2023-09-12T07:12:44.629924Z","shell.execute_reply":"2023-09-12T07:12:44.629946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 20. Overview of DistilBERT model\n\nDistilBERT is a `lightweight` and more `efficient` compare than the original BERT  model. It was introduced by `Hugging-Face` in their paper titled \"DistilBERT, a distilled version of BERT\" in `2019`.\n* DistilBERT has the `same` general architecture as BERT\n* `DistilBERT` is `40% smaller` and `60% faster` compare than original BERT model. so that reason distilBERT known as student and BERT as teacher.\n* DistilBERT can understand the context of a word by considering both the `left` and `right` surrounding words in a sentence. \n*  It is first pre-trained on a `large corpus of text data` and then `fine-tuned` on specific tasks such as `classification` or `named entity recognition(NER)`.\n* it also use **Masked Language Model** `(MLM)` masking technique during pre-training.\n* It achieves this by `reducing` the number of layers and the `hidden size` of the model.\n\n\n* The BERT-base model has 12 layers, while DistilBERT has 6 layers.\n* **Token type embeddings are removed:** Token type embeddings are used to distinguish between different tokens in a sentence, such as the difference between a `noun` and a `verb`. DistilBERT does not use token type embeddings, which reduces the number of parameters.\n* **The pooler is removed.** The pooler is a layer that is used to extract a representation of the entire sentence. DistilBERT does not use the pooler, which further reduces the number of parameters.\n\n### Refrence:  [DistilBERT, a distilled version of BERT](https://arxiv.org/pdf/1910.01108v4.pdf)\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 21. Model Implementation - DistilBERT\n\n- `DistilBertTokenizer` loads the pre-trained tokenizer corresponding to the `distilbert-base-uncased` variant. This tokenizer is responsible for converting text data into numerical tokens processed by the model.\n\n- We provide it with a list of text samples that the `DistilBERT tokenizer` tokenizes.\n\n- `truncation=True`: This argument specifies that if a text is longer than the maximum token length (512 tokens in this case), it will be truncated. \n\n- `padding=True`: This adds padding to the text, ensuring it has a uniform length. DistilBERT picks up to `512` tokens and pads if there are fewer.\n\n- `train_encodings` will contain the token IDs and attention masks for the training data.\n\n- Dataset is created using `TensorDataset`.\n\n- `train_encodings['input_ids']` contains the token IDs of the training data after tokenization.\n\n- `train_encodings['attention_mask']` contains the attention masks which indicate which tokens are actual words and which are padding tokens.\n\n- `torch.tensor(train_labels)` includes the labels associated with the training data. `TensorDataset` combines `train_encodings['input_ids']` and `train_encodings['attention_mask']`, wrapping each sample.\n\nThis setup allows us to efficiently use the DistilBERT model for tasks such as classification, sentiment analysis, and more, by leveraging the token IDs and attention masks produced by the tokenizer.\n","metadata":{}},{"cell_type":"code","source":"# Change tokenizer\ntokenizer_distil = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\ntrain_encodings_distil = tokenizer_distil(train_data, truncation=True, padding=True, max_length=512, return_tensors='pt')\ntest_encodings_distil = tokenizer_distil(test_data, truncation=True, padding=True, max_length=512, return_tensors='pt')\n\ntrain_dataset_distil = TensorDataset(train_encodings_distil['input_ids'], train_encodings_distil['attention_mask'], torch.tensor(train_labels))\ntest_dataset_distil = TensorDataset(test_encodings_distil['input_ids'], test_encodings_distil['attention_mask'], torch.tensor(test_labels))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:05:26.457454Z","iopub.execute_input":"2023-09-12T08:05:26.457839Z","iopub.status.idle":"2023-09-12T08:06:13.057555Z","shell.execute_reply.started":"2023-09-12T08:05:26.457804Z","shell.execute_reply":"2023-09-12T08:06:13.056549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 22. Create the Dataloader - DistilBERT\n- In which we set the `batch-size` to `16`. means that during the training we feed to model for processing 16 at a time.\n  * we set different batch size if we exceeding to 16 accuring the memory error your memory is not free etc. so that we set     16 samples at a time according to our computational resources.\n  \n* In first we pass wrap-up data that above created mean it`train_encodings['input_ids']` and          `train_encodings['attention_mask']` and its label.\n\n* Second we give batch-size that we define is `16`.\n\n* `shuffl=true` mean that shuffling the data at the each epochs. with this we feed whole data to the model and understand      the data contextually.\n\n\n\n","metadata":{}},{"cell_type":"code","source":"# Set up DataLoader for DistilBERT\nbatch_size_distil = 16\ntrain_loader_distil = DataLoader(train_dataset_distil, batch_size=batch_size_distil, shuffle=True)\ntest_loader_distil = DataLoader(test_dataset_distil, batch_size=batch_size_distil)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:06:13.059624Z","iopub.execute_input":"2023-09-12T08:06:13.060070Z","iopub.status.idle":"2023-09-12T08:06:13.067573Z","shell.execute_reply.started":"2023-09-12T08:06:13.060035Z","shell.execute_reply":"2023-09-12T08:06:13.064746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 23. Model Loading - DistilBERT\n\n* load the `DistilBertForSequenceClassification` pre-trained `distilbert-base-uncased` model with 2 labels and tarined on own corpus.\n\n- We use `Adam` optimizer with learning rate [0.01,0.25] both lr use but not any effect on results.\n   * Adam adapts the learning rates for each parameter based on their previous gradients. It maintains a per-parameter            learning rate that is adjusted during training.\n   \n- `torch.device` used for check the if the GPU is available and model set on device `GPU` otherwise use CPU.","metadata":{}},{"cell_type":"code","source":"# Initialize and configure the DistilBERT model for sequence classification\nmodel_distil = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\noptimizer_distil = torch.optim.AdamW(model_distil.parameters(), lr=2e-5)\nscheduler_distil = get_linear_schedule_with_warmup(optimizer_distil, num_warmup_steps=len(train_loader_distil) * 1, num_training_steps=len(train_loader_distil) * 10)\n\n# Training loop for DistilBERT\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_distil.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:06:13.068921Z","iopub.execute_input":"2023-09-12T08:06:13.069196Z","iopub.status.idle":"2023-09-12T08:06:15.041643Z","shell.execute_reply.started":"2023-09-12T08:06:13.069159Z","shell.execute_reply":"2023-09-12T08:06:15.040601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 24. Model training - DistilBERT\n\n- Using loop It trains the model for the specified number of epochs.\n * intially get better result becuase we chose `10 epoch` first. we change number of 5,10,15,20 even run the `10 epochs` but not        effect on results. so that also these reason `we chose 10 epochs.`\n \n- Using the `tqdm` library visulize the model running time and running progress. \n\n- Computes training `losses` and `accuracies`.\n\n- Saves the model after each epoch.","metadata":{}},{"cell_type":"code","source":"epochs_distil = 10\ntraining_accuracies_distil = []\ntraining_losses_distil = []\n\noutput_dir_distil = \"distilBERT_5e\"\nos.makedirs(output_dir_distil, exist_ok=True)\n\nfor epoch in range(epochs_distil):\n    model_distil.train()\n    train_loss_distil = 0.0\n    correct_distil = 0\n    total_distil = 0\n    \n    for batch in tqdm(train_loader_distil, desc=f\"Epoch {epoch + 1}\"):\n        input_ids, attention_mask, labels = batch\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n        \n        optimizer_distil.zero_grad()\n        outputs = model_distil(input_ids, attention_mask=attention_mask, labels=labels)\n        \n        # Use the loss from the model\n        loss = outputs.loss\n        train_loss_distil += loss.item()\n        \n        # Compute accuracy\n        logits = outputs.logits\n        _, predicted = torch.max(logits, 1)\n        total_distil += labels.size(0)\n        correct_distil += (predicted == labels).sum().item()\n        \n        loss.backward()\n        optimizer_distil.step()\n        scheduler_distil.step()\n\n    avg_train_loss_distil = train_loss_distil / len(train_loader_distil)\n    training_losses_distil.append(avg_train_loss_distil)\n    \n    # Calculate training accuracy\n    train_accuracy_distil = correct_distil / total_distil\n    training_accuracies_distil.append(train_accuracy_distil)\n    \n    print(f\"Epoch {epoch + 1} - Average training loss: {avg_train_loss_distil:.4f}, Training accuracy: {train_accuracy_distil:.4f}\")\n    \n    # Save the model after every epoch in the specified directory\n    model_distil.save_pretrained(os.path.join(output_dir_distil, f\"distilbert_epoch_{epoch + 1}_16_adam\"))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:06:15.044291Z","iopub.execute_input":"2023-09-12T08:06:15.045370Z","iopub.status.idle":"2023-09-12T08:28:22.809906Z","shell.execute_reply.started":"2023-09-12T08:06:15.045334Z","shell.execute_reply":"2023-09-12T08:28:22.808887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visulize the accuracy of each epoch","metadata":{}},{"cell_type":"code","source":"\n# Data\nepochs = range(1, len(training_accuracies_distil) + 1)\n\n# Create subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Training Accuracy plot\nax1.plot(epochs, training_accuracies_distil, marker='o', color='#1f77b4', label='Training Accuracy')\nax1.set_title('Training Accuracy Over Epochs')\nax1.set_xlabel('Epochs')\nax1.set_ylabel('distilBERT Accuracy')\nax1.set_xticks(range(1, len(training_accuracies) + 1))\nax1.legend()\n\n# Training Loss plot\nax2.plot(epochs, training_losses_distil, marker='o', color='#1f77b4', label='Training Loss')\nax2.set_title('Training Loss Over Epochs')\nax2.set_xlabel('Epochs')\nax2.set_ylabel('disrilBERT Loss')\nax2.set_xticks(range(1, len(training_accuracies) + 1))\nax2.legend()\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:25:48.562926Z","iopub.execute_input":"2023-09-12T13:25:48.563613Z","iopub.status.idle":"2023-09-12T13:25:49.256934Z","shell.execute_reply.started":"2023-09-12T13:25:48.563578Z","shell.execute_reply":"2023-09-12T13:25:49.255984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 25. Model Evaluation - DistilBERT\n\n- After model is trained and **evaluate** the model that how many trained well our model.\n\n- At the end print the **Confusion Matrics**.","metadata":{}},{"cell_type":"code","source":"# Evaluation loop for DistilBERT\nmodel_distil.eval()\ny_true_distil = []\ny_pred_distil = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader_distil, desc=\"Evaluating\"):\n        input_ids, attention_mask, labels = batch\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n\n        outputs = model_distil(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        predicted_labels = np.argmax(logits.cpu().numpy(), axis=1)\n\n        y_true_distil.extend(labels.cpu().numpy())\n        y_pred_distil.extend(predicted_labels)\n\n# Print classification metrics\nprint(classification_report(y_true_distil, y_pred_distil, target_names=[\"Negative\", \"Positive\"]))\nconf_matrix_distil = confusion_matrix(y_true_distil, y_pred_distil)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix_distil)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:37:40.426125Z","iopub.execute_input":"2023-09-12T08:37:40.426483Z","iopub.status.idle":"2023-09-12T08:37:44.819337Z","shell.execute_reply.started":"2023-09-12T08:37:40.426451Z","shell.execute_reply":"2023-09-12T08:37:44.818343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 26. Visualize the Resuts\n* Plot the accuracy of both model of each epoch.","metadata":{}},{"cell_type":"code","source":"epochs = range(1, 11)\n\n# Create subplots for accuracy and loss\nplt.figure(figsize=(14, 5))\n\n# Accuracy subplot\nplt.subplot(1, 2, 1)\nplt.plot(epochs, training_accuracies, marker='o', color='#1f77b4', label='DistilBERT')\nplt.plot(epochs, training_accuracies_distil, marker='o', color='#ff7f0e', label='RoBERTa')\nplt.xticks(range(1, len(training_accuracies) + 1))\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model Accuracy')\nplt.legend()\n\n# Loss subplot\nplt.subplot(1, 2, 2)\nplt.plot(epochs, training_losses, marker='o', color='#1f77b4', label='DistilBERT')\nplt.plot(epochs, training_losses_distil, marker='o', color='#ff7f0e', label='RoBERTa')\nplt.xticks(range(1, len(training_accuracies) + 1))\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Model Loss')\nplt.legend()\n\nplt.tight_layout()\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:49:16.049332Z","iopub.execute_input":"2023-09-12T08:49:16.050015Z","iopub.status.idle":"2023-09-12T08:49:16.642497Z","shell.execute_reply.started":"2023-09-12T08:49:16.049977Z","shell.execute_reply":"2023-09-12T08:49:16.641587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Just show accuracy of both model using Bar-plot","metadata":{}},{"cell_type":"code","source":"# Data\nmodels = ['RoBERTa', 'DistilBERT']\naccuracies = [93, 91]\n\n# Create the bar chart\nplt.figure(figsize=(8, 6))\nplt.bar(models, accuracies, color=['#1f77b4', '#ff7f0e'])\n\nplt.xlabel('Models')\nplt.ylabel('Accuracy (%)')\nplt.title('Validation Accuracy Comparison between RoBERTa and DistilBERT')\n\n# Set y-axis ticks with a difference of 10\nplt.yticks(range(0, 101, 10))\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:26:10.556069Z","iopub.execute_input":"2023-09-12T13:26:10.556448Z","iopub.status.idle":"2023-09-12T13:26:10.804166Z","shell.execute_reply.started":"2023-09-12T13:26:10.556417Z","shell.execute_reply":"2023-09-12T13:26:10.803220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 27. RoBERTa vs. BERT\n\n* **Accuracy Improvement:** `roBERTa` and `distilBERT` models show very slightly improvements in\naccuracy as training progresses over the epochs.\n\n* **Training aspect:** `roBERTa` require much long time for training compare than `distilBERT` training time very faster. Both models achieve in initially epoch accuracy is not good but over the epochs improve itself slightly.\n\n* **Model Preference:** `reBERTa` give 98% accuracy on training and 93%non validation. In `distilBERT` give 99% on training and 90% accuracy on validation.\n\n* **Overfitting:** both models is going to overfitting.","metadata":{}},{"cell_type":"markdown","source":"# 28. Conclusion\n\nIn conclusion, `RoBERTa` appears to be the stronger performer in this `sentiment analysis` task,\nshowing higher **precision, recall, F1-scores**, and **accuracy** compared to distilBERT on the test dataset.\nIt is recommended for use in this specific NLP application.\n\n                                                \n#                                                   **THE END!**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}